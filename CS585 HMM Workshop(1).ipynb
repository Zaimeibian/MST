{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 585: Natural Langugae Processing \n",
    "\n",
    "## HMM for POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import os.path\n",
    "import urllib.request\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    url = 'https://www.dropbox.com/s/ty7cclxiob3ajog/data.txt?dl=1'\n",
    "    urllib.request.urlretrieve(url, 'data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_labeled_data(filename):\n",
    "\n",
    "    \"\"\"\n",
    "        Read Training Data\n",
    "    \"\"\"\n",
    "\n",
    "    file = open(filename)\n",
    "    Text = file.read()\n",
    "    text = Text.strip('\\n').split('\\n')\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    sentence = []\n",
    "    tag = []\n",
    "    for text1 in text:\n",
    "        l = text1.split(' ')\n",
    "        if (len(l) == 2):\n",
    "            sentence.append(l[0])\n",
    "            tag.append(l[1])\n",
    "        else:\n",
    "            sentences.append(sentence)\n",
    "            tags.append(tag)\n",
    "            sentence = []\n",
    "            tag = []\n",
    "    return sentences,tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self,smoothing):\n",
    "        self.smoothing = smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_bigrams(l=[]):\n",
    "        return (l[i:i + 2] for i in range(len(l) - 1))\n",
    "HMM.iter_bigrams = iter_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_transition_probas(self, tags):\n",
    "    '''\n",
    "    Compute the transition probabilities.\n",
    "    '''\n",
    "    self.transition_probas = defaultdict(lambda: Counter())\n",
    "    for tag in tags:\n",
    "        for bigram in iter_bigrams(tag):\n",
    "            self.transition_probas[bigram[0]].update([bigram[-1]])\n",
    "    unique_tags = set(x for l in tags for x in l)\n",
    "    \n",
    "    self.states = list(unique_tags)\n",
    "    \n",
    "    for tag,tag_counts in self.transition_probas.items():\n",
    "        total = sum(tag_counts.values())\n",
    "        for i in self.transition_probas.values():\n",
    "            for j in unique_tags:\n",
    "                if(j not in i):\n",
    "                    i.update({j:0})\n",
    "        self.transition_probas[tag] = {word: count/total for word,count in tag_counts.items()}\n",
    "HMM.fit_transition_probas = fit_transition_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_emission_probas(self, sentences, tags):\n",
    "    '''\n",
    "    Compute the emission probabilities\n",
    "    '''\n",
    "    self.emission_probas = defaultdict(lambda: Counter())\n",
    "    for tag,sentence in zip(tags,sentences):\n",
    "        for t,s in zip(tag,sentence):\n",
    "            for bigram in iter_bigrams([t,s]):\n",
    "                self.emission_probas[bigram[0]].update([bigram[-1]])\n",
    "    \n",
    "    unique_words = set(x for l in self.emission_probas.values() for x in l)\n",
    "    \n",
    "    for tag,word_counts in self.emission_probas.items():\n",
    "        total = sum(word_counts.values())\n",
    "        for i in self.emission_probas.values():\n",
    "            for j in unique_words:\n",
    "                if (j not in i):\n",
    "                    i.update({j:0})\n",
    "        self.emission_probas[tag] = {w: count/total for w,count in word_counts.items()}\n",
    "                \n",
    "HMM.fit_emission_probas = fit_emission_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_start_probas(self, tags):\n",
    "    '''\n",
    "    Compute the start state probs\n",
    "    '''\n",
    "    counts = Counter(tag[0] for tag in tags)\n",
    "    total = sum(counts.values())\n",
    "    self.start_probas = {w: counts[w]/total for w in self.states}\n",
    "HMM.fit_start_probas = fit_start_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(self,sentences,tags):\n",
    "    '''\n",
    "    Fit the model to the training data\n",
    "    '''\n",
    "    self.fit_transition_probas(tags)\n",
    "    self.fit_emission_probas(sentences,tags)\n",
    "    self.fit_start_probas(tags)\n",
    "HMM.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi(self,sentence):\n",
    "    '''\n",
    "    Compute Hidden POS sequence using viterbi algorithm\n",
    "    '''\n",
    "\n",
    "    states = self.states\n",
    "    viterbi = np.zeros((len(states),len(sentence)))\n",
    "    backp = np.zeros((len(states),len(sentence)),np.int)\n",
    "    \n",
    "    for i in range(len(states)):\n",
    "        viterbi[i][0] = self.start_probas[states[i]] * self.emission_probas[states[i]][sentence[0]]\n",
    "        backp[i][0] = 0\n",
    "    \n",
    "    for i in range(1,len(sentence)):\n",
    "        for post_state in range(len(states)):\n",
    "            v_max = []\n",
    "            b_max = []\n",
    "            for pre_state in range(len(states)):\n",
    "                v_max.append(viterbi[pre_state][i-1]* self.transition_probas[states[pre_state]][states[post_state]] \\\n",
    "                            * self.emission_probas[states[post_state]][sentence[i]])\n",
    "                b_max.append(viterbi[pre_state][i-1] * self.transition_probas[states[pre_state]][states[post_state]])\n",
    "            viterbi[post_state][i] = max(v_max)\n",
    "            backp[post_state][i] = np.argmax(b_max)\n",
    "        \n",
    "    end_state = np.argmax(viterbi[:,len(sentence)-1])\n",
    "    cur_state = backp[end_state][len(sentence)-1]\n",
    "    path = [end_state]\n",
    "    i = len(sentence) - 2\n",
    "    while i!=-1:\n",
    "        path.insert(0,cur_state)\n",
    "        cur_state = backp[cur_state][i]\n",
    "        i -= 1\n",
    "    \n",
    "    return list(np.array(states)[path]),np.max(viterbi[:,len(sentence)-1])\n",
    "HMM.viterbi = viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 34 states\n",
      "['NNP', 'TO', '$', 'NNPS', 'IN', 'VBP', 'PRP$', 'EX', 'NN', 'JJ', '.', 'VBG', 'VBZ', 'PRP', 'MD', 'WRB', ':', ',', 'VBN', 'NNS', 'RB', 'JJR', 'JJS', 'CC', 'WDT', 'CD', 'WP', 'RBR', 'POS', 'DT', 'VBD', \"''\", 'VB', '``']\n",
      "predicted parts of speech for the sentence ['Look', 'at', 'what', 'happened']\n",
      "(['VB', 'IN', 'WP', 'VBD'], 3.3172212325675426e-10)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    M\n",
    "    '''\n",
    "    fname = 'data.txt'\n",
    "    if not os.path.isfile(fname):\n",
    "        download_data()\n",
    "    sentences, tags = read_labeled_data(fname)\n",
    "\n",
    "    model = HMM(0.01)\n",
    "    model.fit(sentences, tags)\n",
    "    print('model has %d states' % len(model.states))\n",
    "    print(model.states)\n",
    "    sentence = nltk.word_tokenize(\"Look at what happened\")\n",
    "    print('predicted parts of speech for the sentence %s' % str(sentence))\n",
    "    print(model.viterbi(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
